{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and subset Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"../data_intermed/news_bert.csv\")\n",
    "news = news[news['text']!=\" \"] # remove empty entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilBert_title_raw = np.load('../data_intermed/distilBert_title.npy')\n",
    "distilBert_text_raw = np.load('../data_intermed/distilBert_text.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLabels(data, col_name):\n",
    "    labels = data[col_name].values\n",
    "    y = np.zeros(labels.shape)\n",
    "    y[labels == 'fake'] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44271, 768)\n"
     ]
    }
   ],
   "source": [
    "distilBert_title = distilBert_title_raw[news.index]\n",
    "distilBert_text = distilBert_text_raw[news.index]\n",
    "y = createLabels(news, 'label')\n",
    "\n",
    "print(distilBert_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Subset Data to only politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['politicsNews', 'worldnews', 'News', 'politics', 'Government News',\n",
       "       'left-news', 'US_News', 'Middle-east'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_p = news[news['subject'].isin(['politicsNews','politics','Government News','left-news'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "fake    12244\n",
       "true    11271\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_p.size\n",
    "news_p.groupby('label').count().title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23515, 768)\n"
     ]
    }
   ],
   "source": [
    "distilBert_title_p = distilBert_title[news_p.index]\n",
    "distilBert_text_p = distilBert_text[news_p.index]\n",
    "y_p = createLabels(news_p, 'label')\n",
    "\n",
    "print(distilBert_text_p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilBert_title_text = np.concatenate((distilBert_title, distilBert_text),1)\n",
    "distilBert_title_text_p = np.concatenate((distilBert_title_p, distilBert_text_p),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44271, 1536)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilBert_title_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEval(X_train, y_train, X_test, y_test, model, result_output_name, path = \"../model_results/\", printResults = True):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_perc, random_state=42)\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    assert(y_pred_train.shape == y_train.shape)\n",
    "    assert(y_pred_test.shape == y_test.shape)\n",
    "    \n",
    "    # save results in dictionary\n",
    "    model_dict = {}\n",
    "    model_dict['train accuracy'] = clf.score(X_train, y_train)\n",
    "    model_dict['test accuracy'] = clf.score(X_test, y_test)\n",
    "    model_dict['train f-score'] = f1_score(y_train, y_pred_train)\n",
    "    model_dict['test f-score'] = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    # output the dictionary\n",
    "    with open(path + result_output_name + \".json\", \"w\") as outfile:  \n",
    "        json.dump(model_dict, outfile) \n",
    "    \n",
    "    # print\n",
    "    if printResults == True:\n",
    "        print('train accuracy:', model_dict['train accuracy'])\n",
    "        print('test accuracy:', model_dict['test accuracy'])\n",
    "        print('train f-score:', model_dict['train f-score'])\n",
    "        print('test f-score:', model_dict['test f-score'])\n",
    "    \n",
    "    return clf, model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['title','text','title_text']:\n",
    "    exec(\"X_train_\"+i+\",X_test_\"+i+\",y_train_\"+i+\",y_test_\"+i+\n",
    "         \"= train_test_split(distilBert_\" +i + \",y, test_size=0.33, random_state=42)\")\n",
    "    exec(\"X_train_\"+i+\"_p ,X_test_\"+i+\"_p ,y_train_\"+i+\"_p ,y_test_\"+i+\n",
    "         \"_p = train_test_split(distilBert_\" +i + \"_p ,y_p, test_size=0.33, random_state=42)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes (need to update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8516570580897475\n",
      "test accuracy: 0.8499657768651608\n",
      "train f-score: 0.8537914534458695\n",
      "test f-score: 0.8528859060402684\n"
     ]
    }
   ],
   "source": [
    "clf_nb_title = modelEval(X_train_title, y_train_title, X_test_title, y_test_title,\n",
    "                         gnb, \"results_nb_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9221199554971174\n",
      "test accuracy: 0.9243668720054757\n",
      "train f-score: 0.924809582709459\n",
      "test f-score: 0.9270193514298922\n"
     ]
    }
   ],
   "source": [
    "clf_nb_text = modelEval(X_train_text, y_train_text, X_test_text, y_test_text, \n",
    "                        gnb, \"results_nb_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9204005259431577\n",
      "test accuracy: 0.920123203285421\n",
      "train f-score: 0.9226434258379477\n",
      "test f-score: 0.9223191106969314\n"
     ]
    }
   ],
   "source": [
    "clf_nb_title_text = modelEval(X_train_title_text, y_train_title_text, X_test_title_text, y_test_title_text,\n",
    "                              gnb, \"results_nb_title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8703268803554427\n",
      "test accuracy: 0.8743556701030928\n",
      "train f-score: 0.8731607375675172\n",
      "test f-score: 0.8778654641112363\n"
     ]
    }
   ],
   "source": [
    "clf_nb_title_p = modelEval(X_train_title_p, y_train_title_p, X_test_title_p, y_test_title_p,\n",
    "                           gnb, \"results_nb_title_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9178673437004126\n",
      "test accuracy: 0.9252577319587629\n",
      "train f-score: 0.9189426208970183\n",
      "test f-score: 0.9268046441191318\n"
     ]
    }
   ],
   "source": [
    "clf_nb_text_p = modelEval(X_train_text_p, y_train_text_p, X_test_text_p, y_test_text_p,\n",
    "                          gnb, \"results_nb_text_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9294827039035227\n",
      "test accuracy: 0.9371134020618557\n",
      "train f-score: 0.9311946491608348\n",
      "test f-score: 0.938877755511022\n"
     ]
    }
   ],
   "source": [
    "clf_nb_title_text_p = modelEval(X_train_title_text_p, y_train_title_text_p, X_test_title_text_p, y_test_title_text_p, \n",
    "                                gnb, \"results_nb_title_text_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "def crossValidationLR(X, y, hyperparameters):\n",
    "    scores = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    for h in hyperparameters:\n",
    "        model = LogisticRegression(random_state=0, max_iter=2000, C=h)\n",
    "        scores.append(cross_val_score(estimator=model, X=X_train, y=y_train, cv=10).mean())\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "scoresLR_title_text = crossValidationLR(X_train_title_text, y_train_title_text, range(1,10))\n",
    "scoresLR_title_text_p = crossValidationLR(X_train_title_text_p, y_train_title_text_p, range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9940116795387322,\n",
       "  0.9942129880440165,\n",
       "  0.9942632898548815,\n",
       "  0.9943135916657468,\n",
       "  0.9943135916657468,\n",
       "  0.9943639187920678,\n",
       "  0.9943135916657468,\n",
       "  0.9942129374131046,\n",
       "  0.9942129374131046],\n",
       " [0.9966841698980323,\n",
       "  0.9968736535975872,\n",
       "  0.9966840801378716,\n",
       "  0.9966839903777107,\n",
       "  0.9967786873474077,\n",
       "  0.9968733843171046,\n",
       "  0.9968733843171046,\n",
       "  0.9968733843171046,\n",
       "  0.9967786873474077])"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoresLR_title_text, scoresLR_title_text_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the lr that is the best\n",
    "lr = LogisticRegression(random_state=0, max_iter = 2000, C=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9730285560163178\n",
      "test accuracy: 0.9679671457905544\n",
      "train f-score: 0.9738014147236049\n",
      "test f-score: 0.9688664183076103\n"
     ]
    }
   ],
   "source": [
    "clf_lr_title = modelEval(X_train_title, y_train_title, X_test_title, y_test_title, lr, \"results_lr_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9963925693671825\n",
      "test accuracy: 0.9907597535934292\n",
      "train f-score: 0.9965045245173303\n",
      "test f-score: 0.9910423993099331\n"
     ]
    }
   ],
   "source": [
    "clf_lr_text = modelEval(X_train_text, y_train_text, X_test_text, y_test_text, lr, \"results_lr_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9999662856950204\n",
      "test accuracy: 0.9964407939767282\n",
      "train f-score: 0.9999673512031081\n",
      "test f-score: 0.9965508092332185\n"
     ]
    }
   ],
   "source": [
    "clf_lr_title_text = modelEval(X_train_title_text, y_train_title_text, X_test_title_text, y_test_title_text, \n",
    "                              lr, \"results_lr_title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9859727070771184\n",
      "test accuracy: 0.9779639175257732\n",
      "train f-score: 0.9864823536607744\n",
      "test f-score: 0.9788654060066742\n"
     ]
    }
   ],
   "source": [
    "clf_lr_title_p = modelEval(X_train_title_p, y_train_title_p, X_test_title_p, y_test_title_p,\n",
    "                           lr, \"results_lr_title_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9998095842589654\n",
      "test accuracy: 0.9949742268041237\n",
      "train f-score: 0.9998170173833487\n",
      "test f-score: 0.9951810206351168\n"
     ]
    }
   ],
   "source": [
    "clf_lr_text_p = modelEval(X_train_text_p, y_train_text_p, X_test_text_p, y_test_text_p, \n",
    "                          lr, \"results_lr_text_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy: 0.9983247422680412\n",
      "train f-score: 1.0\n",
      "test f-score: 0.9983932764800395\n"
     ]
    }
   ],
   "source": [
    "clf_lr_title_text_p = modelEval(X_train_title_text_p, y_train_title_text_p, X_test_title_text_p, y_test_title_text_p, \n",
    "                                lr, \"results_lr_title_text_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC #l2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "def crossValidationLSVM(X, y, hyperparameters):\n",
    "    scores = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    for h in hyperparameters:\n",
    "        model = LinearSVC(max_iter = 50000, C=h)\n",
    "        scores.append(cross_val_score(estimator=model, X=X_train, y=y_train, cv=10).mean())\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "scoresLSVM_title_text = crossValidationLSVM(X_train_title_text, y_train_title_text, range(1,10))\n",
    "scoresLSVM_title_text_p = crossValidationLSVM(X_train_title_text_p, y_train_title_text_p, range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9935587860327543,\n",
       "  0.9933575281583817,\n",
       "  0.9933575281583817,\n",
       "  0.9932568992211953,\n",
       "  0.9931562702840091,\n",
       "  0.9931562702840091,\n",
       "  0.9931562702840091,\n",
       "  0.9931562702840091,\n",
       "  0.9931059431576881],\n",
       " [0.9968735638374264,\n",
       "  0.9968733843171048,\n",
       "  0.9968733843171048,\n",
       "  0.9968733843171048,\n",
       "  0.9968733843171048,\n",
       "  0.9969680812868017,\n",
       "  0.9969680812868017,\n",
       "  0.9969680812868017,\n",
       "  0.9969680812868017])"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoresLSVM_title_text, scoresLSVM_title_text_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(max_iter = 10000, C=1)\n",
    "lsvc_p = LinearSVC(max_iter = 50000, C=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9732982704561546\n",
      "test accuracy: 0.9679671457905544\n",
      "train f-score: 0.9740566037735848\n",
      "test f-score: 0.9688581314878894\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_title = modelEval(X_train_title, y_train_title, X_test_title, y_test_title, \n",
    "                           lsvc, \"results_lsvm_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9976062843464482\n",
      "test accuracy: 0.9909650924024641\n",
      "train f-score: 0.9976811783533099\n",
      "test f-score: 0.9912443618997081\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_text = modelEval(X_train_text, y_train_text, X_test_text, y_test_text, \n",
    "                          lsvc, \"results_lsvm_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy: 0.9960301163586585\n",
      "train f-score: 1.0\n",
      "test f-score: 0.9961553758451545\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_title_text = modelEval(X_train_title_text, y_train_title_text, X_test_title_text, y_test_title_text, \n",
    "                                lsvc, \"results_lsvm_title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9902887972072358\n",
      "test accuracy: 0.9740979381443299\n",
      "train f-score: 0.9906530637180035\n",
      "test f-score: 0.975255447494768\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_title_p = modelEval(X_train_title_p, y_train_title_p, X_test_title_p, y_test_title_p, \n",
    "                             lsvc_p, \"results_lsvm_title_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy: 0.9942010309278351\n",
      "train f-score: 1.0\n",
      "test f-score: 0.9944396391943655\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_text_p = modelEval(X_train_text_p, y_train_text_p, X_test_text_p, y_test_text_p, \n",
    "                            lsvc_p, \"results_lsvm_text_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy: 0.997680412371134\n",
      "train f-score: 1.0\n",
      "test f-score: 0.9977755808205635\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_title_text_p = modelEval(X_train_title_text_p, y_train_title_text_p, X_test_title_text_p, y_test_title_text_p,\n",
    "                                  lsvc_p, \"results_lsvm_title_text_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Neural Network (rerun with dropout 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, features, labels): #labels will be numpy (n,)\n",
    "        'Initialization'\n",
    "        self.features = torch.tensor(features, dtype=torch.float) #instance variables\n",
    "        self.labels = torch.tensor(labels.reshape(-1,1), dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        X = self.features[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def getFeatures(self):\n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-711-d78502fc918a>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-711-d78502fc918a>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    nn.Dropout(0.2),\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# create model class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplr neural network to intake bert\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden = 700\n",
    "        self.l1 = nn.Linear(p, self.hidden, bias=True)  \n",
    "        self.l2 = nn.Linear(self.hidden, 1, bias=True) \n",
    "        \n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1, \n",
    "            nn.Tanh(), \n",
    "            nn.BatchNorm1d()\n",
    "            nn.Dropout(0.2), \n",
    "            self.l2, \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for training (similar to hw8)\n",
    "def train(X, y, X_test, y_test, model, batch_size, num_epochs, criterion, optimizer):\n",
    "    loss_curve = []\n",
    "    accuracy_curve = []\n",
    "    \n",
    "    # create train test split\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    dataloader = DataLoader(Dataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs): # loop over each epoch\n",
    "        epoch_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for data in dataloader: # loop over each batch\n",
    "            embeddings, labels = data\n",
    "            outputs = model(embeddings.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save train performance\n",
    "            epoch_loss+=loss.item()\n",
    "            preds = 1*(outputs>0.5)\n",
    "            correct += (preds.reshape(-1,1) == labels).sum().item()\n",
    "            total += float(len(labels))\n",
    "        \n",
    "        # train loss and accuracy\n",
    "        epoch_loss = epoch_loss / len(dataloader)\n",
    "        epoch_accuracy = correct / total\n",
    "        loss_curve.append(epoch_loss)\n",
    "        accuracy_curve.append(epoch_accuracy)\n",
    "        \n",
    "        # validation accuracy \n",
    "        X_val_t = torch.tensor(X_val, dtype=torch.float) #instance variables\n",
    "        y_val_t = torch.tensor(y_val.reshape(-1,1), dtype=torch.float)\n",
    "        val_outputs = model(X_val_t.float())\n",
    "        val_preds = 1*(val_outputs>0.5)\n",
    "        val_accuracy = (val_preds.reshape(-1,1) == y_val_t).sum().item()/len(y_val)\n",
    "        \n",
    "        # test accuracy \n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float) #instance variables\n",
    "        y_test_t = torch.tensor(y_test.reshape(-1,1), dtype=torch.float)\n",
    "        test_outputs = model(X_test_t.float())\n",
    "        test_preds = 1*(test_outputs>0.5)\n",
    "        test_accuracy = (test_preds.reshape(-1,1) == y_test_t).sum().item()/len(y_test)\n",
    "        \n",
    "        print('epoch [{}/{}], mean epoch loss:{:.4f}, train acc:{:.4f}, val acc:{:.4f}, test acc:{:.4f}'.format(\n",
    "            epoch + 1, num_epochs, epoch_loss, epoch_accuracy, val_accuracy, test_accuracy))\n",
    "        \n",
    "    return model, loss_curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate other inputs\n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "model = NeuralNetwork(X_train_text.shape[1])\n",
    "model_title_text = NeuralNetwork(X_train_title_text.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer_title_text = optim.Adam(model_title_text.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], mean epoch loss:0.1205, train acc:0.9600, val acc:0.9776, test acc:0.9738\n",
      "epoch [2/20], mean epoch loss:0.0736, train acc:0.9753, val acc:0.9843, test acc:0.9802\n",
      "epoch [3/20], mean epoch loss:0.0602, train acc:0.9787, val acc:0.9826, test acc:0.9810\n",
      "epoch [4/20], mean epoch loss:0.0513, train acc:0.9820, val acc:0.9848, test acc:0.9839\n",
      "epoch [5/20], mean epoch loss:0.0494, train acc:0.9818, val acc:0.9675, test acc:0.9679\n",
      "epoch [6/20], mean epoch loss:0.0435, train acc:0.9845, val acc:0.9870, test acc:0.9853\n",
      "epoch [7/20], mean epoch loss:0.0396, train acc:0.9855, val acc:0.9879, test acc:0.9851\n",
      "epoch [8/20], mean epoch loss:0.0399, train acc:0.9854, val acc:0.9874, test acc:0.9845\n",
      "epoch [9/20], mean epoch loss:0.0354, train acc:0.9878, val acc:0.9804, test acc:0.9779\n",
      "epoch [10/20], mean epoch loss:0.0368, train acc:0.9864, val acc:0.9815, test acc:0.9808\n",
      "epoch [11/20], mean epoch loss:0.0328, train acc:0.9885, val acc:0.9880, test acc:0.9862\n",
      "epoch [12/20], mean epoch loss:0.0391, train acc:0.9863, val acc:0.9810, test acc:0.9814\n",
      "epoch [13/20], mean epoch loss:0.0332, train acc:0.9881, val acc:0.9823, test acc:0.9823\n",
      "epoch [14/20], mean epoch loss:0.0339, train acc:0.9877, val acc:0.9869, test acc:0.9875\n",
      "epoch [15/20], mean epoch loss:0.0301, train acc:0.9896, val acc:0.9880, test acc:0.9855\n",
      "epoch [16/20], mean epoch loss:0.0294, train acc:0.9891, val acc:0.9848, test acc:0.9832\n",
      "epoch [17/20], mean epoch loss:0.0267, train acc:0.9909, val acc:0.9857, test acc:0.9850\n",
      "epoch [18/20], mean epoch loss:0.0294, train acc:0.9891, val acc:0.9838, test acc:0.9804\n",
      "epoch [19/20], mean epoch loss:0.0267, train acc:0.9904, val acc:0.9894, test acc:0.9879\n",
      "epoch [20/20], mean epoch loss:0.0290, train acc:0.9898, val acc:0.9885, test acc:0.9864\n"
     ]
    }
   ],
   "source": [
    "# train models - only using text\n",
    "nn_text, loss_curve_text = train(X_train_text, y_train_text, X_test_text, y_test_text, \n",
    "                                 model, batch_size, num_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], mean epoch loss:0.0886, train acc:0.9659, val acc:0.9884, test acc:0.9882\n",
      "epoch [2/20], mean epoch loss:0.0379, train acc:0.9866, val acc:0.9884, test acc:0.9889\n",
      "epoch [3/20], mean epoch loss:0.0337, train acc:0.9886, val acc:0.9907, test acc:0.9901\n",
      "epoch [4/20], mean epoch loss:0.0260, train acc:0.9910, val acc:0.9926, test acc:0.9925\n",
      "epoch [5/20], mean epoch loss:0.0303, train acc:0.9890, val acc:0.9826, test acc:0.9843\n",
      "epoch [6/20], mean epoch loss:0.0253, train acc:0.9919, val acc:0.9916, test acc:0.9921\n",
      "epoch [7/20], mean epoch loss:0.0210, train acc:0.9928, val acc:0.9917, test acc:0.9913\n",
      "epoch [8/20], mean epoch loss:0.0181, train acc:0.9937, val acc:0.9934, test acc:0.9935\n",
      "epoch [9/20], mean epoch loss:0.0182, train acc:0.9933, val acc:0.9953, test acc:0.9929\n",
      "epoch [10/20], mean epoch loss:0.0174, train acc:0.9941, val acc:0.9938, test acc:0.9924\n",
      "epoch [11/20], mean epoch loss:0.0203, train acc:0.9924, val acc:0.9922, test acc:0.9926\n",
      "epoch [12/20], mean epoch loss:0.0156, train acc:0.9943, val acc:0.9926, test acc:0.9906\n",
      "epoch [13/20], mean epoch loss:0.0159, train acc:0.9946, val acc:0.9917, test acc:0.9918\n",
      "epoch [14/20], mean epoch loss:0.0146, train acc:0.9951, val acc:0.9895, test acc:0.9890\n",
      "epoch [15/20], mean epoch loss:0.0150, train acc:0.9942, val acc:0.9926, test acc:0.9926\n",
      "epoch [16/20], mean epoch loss:0.0166, train acc:0.9936, val acc:0.9917, test acc:0.9921\n",
      "epoch [17/20], mean epoch loss:0.0132, train acc:0.9952, val acc:0.9921, test acc:0.9919\n",
      "epoch [18/20], mean epoch loss:0.0146, train acc:0.9947, val acc:0.9933, test acc:0.9929\n",
      "epoch [19/20], mean epoch loss:0.0149, train acc:0.9944, val acc:0.9870, test acc:0.9861\n",
      "epoch [20/20], mean epoch loss:0.0128, train acc:0.9954, val acc:0.9919, test acc:0.9895\n"
     ]
    }
   ],
   "source": [
    "# train models - using text and title\n",
    "nn_title_text, loss_curve_title_text = train(X_train_title_text, y_train_title_text, X_test_title_text, y_test_title_text,\n",
    "                                             model_title_text, batch_size, num_epochs, criterion, optimizer_title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/20], mean epoch loss:0.0265, train acc:0.9910, val acc:0.9921, test acc:0.9923\n",
      "epoch [2/20], mean epoch loss:0.0198, train acc:0.9925, val acc:0.9956, test acc:0.9932\n",
      "epoch [3/20], mean epoch loss:0.0155, train acc:0.9943, val acc:0.9930, test acc:0.9936\n",
      "epoch [4/20], mean epoch loss:0.0157, train acc:0.9940, val acc:0.9930, test acc:0.9946\n",
      "epoch [5/20], mean epoch loss:0.0143, train acc:0.9952, val acc:0.9946, test acc:0.9938\n",
      "epoch [6/20], mean epoch loss:0.0126, train acc:0.9962, val acc:0.9930, test acc:0.9938\n",
      "epoch [7/20], mean epoch loss:0.0129, train acc:0.9954, val acc:0.9943, test acc:0.9932\n",
      "epoch [8/20], mean epoch loss:0.0183, train acc:0.9929, val acc:0.9937, test acc:0.9934\n",
      "epoch [9/20], mean epoch loss:0.0119, train acc:0.9954, val acc:0.9943, test acc:0.9942\n",
      "epoch [10/20], mean epoch loss:0.0178, train acc:0.9933, val acc:0.9949, test acc:0.9937\n",
      "epoch [11/20], mean epoch loss:0.0108, train acc:0.9964, val acc:0.9927, test acc:0.9928\n",
      "epoch [12/20], mean epoch loss:0.0145, train acc:0.9941, val acc:0.9930, test acc:0.9942\n",
      "epoch [13/20], mean epoch loss:0.0083, train acc:0.9971, val acc:0.9946, test acc:0.9946\n",
      "epoch [14/20], mean epoch loss:0.0091, train acc:0.9970, val acc:0.9937, test acc:0.9938\n",
      "epoch [15/20], mean epoch loss:0.0096, train acc:0.9968, val acc:0.9943, test acc:0.9929\n",
      "epoch [16/20], mean epoch loss:0.0059, train acc:0.9979, val acc:0.9949, test acc:0.9943\n",
      "epoch [17/20], mean epoch loss:0.0059, train acc:0.9982, val acc:0.9940, test acc:0.9929\n",
      "epoch [18/20], mean epoch loss:0.0084, train acc:0.9962, val acc:0.9946, test acc:0.9929\n",
      "epoch [19/20], mean epoch loss:0.0126, train acc:0.9952, val acc:0.9921, test acc:0.9916\n",
      "epoch [20/20], mean epoch loss:0.0079, train acc:0.9971, val acc:0.9914, test acc:0.9903\n"
     ]
    }
   ],
   "source": [
    "# train models - only using text\n",
    "nn_text_p, loss_curve_text_p = train(X_train_text_p, y_train_text_p, X_test_text_p, y_test_text_p, \n",
    "                                 model, batch_size, num_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models - using text and title\n",
    "nn_title_text_p, loss_curve_title_text_p = train(X_train_title_text_p, y_train_title_text_p, X_test_title_text_p, y_test_title_text_p,\n",
    "                                             model_title_text, batch_size, num_epochs, criterion, optimizer_title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LIME on Logistic Regression Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
