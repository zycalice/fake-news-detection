{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and subset Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"../data_intermed/news_bert.csv\")\n",
    "news = news[news['text']!=\" \"] # remove empty entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilBert_title = np.load('../data_intermed/distilBert_title.npy')\n",
    "distilBert_text = np.load('../data_intermed/distilBert_text.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLabels(data, col_name):\n",
    "    labels = data[col_name].values\n",
    "    y = np.zeros(labels.shape)\n",
    "    y[labels == 'fake'] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44271, 768)\n"
     ]
    }
   ],
   "source": [
    "distilBert_title = distilBert_title[news.index]\n",
    "distilBert_text = distilBert_text[news.index]\n",
    "y = createLabels(news, 'label')\n",
    "\n",
    "print(distilBert_text.shape)\n",
    "# print(y.sum(), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Subset Data to only politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['politicsNews', 'worldnews', 'News', 'politics', 'Government News',\n",
       "       'left-news', 'US_News', 'Middle-east'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_p = news[news['subject'].isin(['politicsNews','politics','Government News','left-news'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "fake    12244\n",
       "true    11271\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_p.size\n",
    "news_p.groupby('label').count().title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23515, 768)\n"
     ]
    }
   ],
   "source": [
    "distilBert_title_p = distilBert_title[news_p.index]\n",
    "distilBert_text_p = distilBert_text[news_p.index]\n",
    "y_p = createLabels(news_p, 'label')\n",
    "\n",
    "print(distilBert_text_p.shape)\n",
    "# print(y_p.sum(), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilBert_title_text = np.concatenate((distilBert_title, distilBert_text),1)\n",
    "distilBert_title_text_p = np.concatenate((distilBert_title_p, distilBert_text_p),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44271, 1536)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilBert_title_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEval(X, y, test_perc, model, result_output_name, path = \"../model_results/\", printResults = True):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_perc, random_state=42)\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    assert(y_pred_train.shape == y_train.shape)\n",
    "    assert(y_pred_test.shape == y_test.shape)\n",
    "    \n",
    "    # save results in dictionary\n",
    "    model_dict = {}\n",
    "    model_dict['train accuracy'] = clf.score(X_train, y_train)\n",
    "    model_dict['test accuracy'] = clf.score(X_test, y_test)\n",
    "    model_dict['train f-score'] = f1_score(y_train, y_pred_train)\n",
    "    model_dict['test f-score'] = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    # output the dictionary\n",
    "    with open(path + result_output_name + \".json\", \"w\") as outfile:  \n",
    "        json.dump(model_dict, outfile) \n",
    "    \n",
    "    # print\n",
    "    if printResults == True:\n",
    "        print('train accuracy:', model_dict['train accuracy'])\n",
    "        print('test accuracy:', model_dict['test accuracy'])\n",
    "        print('train f-score:', model_dict['train f-score'])\n",
    "        print('test f-score:', model_dict['test f-score'])\n",
    "    \n",
    "    return clf, model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "def crossValidationLR(X, y, hyperparameters):\n",
    "    scores = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    for h in hyperparameters:\n",
    "        model = LogisticRegression(random_state=0, max_iter=2000, C=h)\n",
    "        scores.append(cross_val_score(estimator=model, X=X_train, y=y_train, cv=10).mean())\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "scores_title_text = crossValidationLR(distilBert_title_text, y, range(1,10))\n",
    "scores_title_text_p = crossValidationLR(distilBert_title_text_p, y_p, range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9953136786058193, 0.9959879760757863],\n",
       " [0.9974612440576907, 0.9977785432277816])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_title_text, scores_title_text_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the lr that is the best\n",
    "lr = LogisticRegression(random_state=0, max_iter = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9698931256532146\n",
      "test accuracy: 0.9671457905544147\n",
      "train f-score: 0.970712669312256\n",
      "test f-score: 0.9680468645985887\n"
     ]
    }
   ],
   "source": [
    "clf_lr_title = modelEval(distilBert_title, y, 0.33, lr, \"results_lr_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9794985718819422\n",
      "test accuracy: 0.9775773195876288\n",
      "train f-score: 0.9802022678516702\n",
      "test f-score: 0.9784119106699752\n"
     ]
    }
   ],
   "source": [
    "clf_lr_title_p = modelEval(distilBert_title_p, y_p, 0.33, lr, \"results_lr_title_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9925154242945282\n",
      "test accuracy: 0.9888432580424367\n",
      "train f-score: 0.9927417772837246\n",
      "test f-score: 0.9891658358258558\n"
     ]
    }
   ],
   "source": [
    "clf_lr_text = modelEval(distilBert_text, y, 0.33, lr, \"results_lr_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9964455728340209\n",
      "test accuracy: 0.993298969072165\n",
      "train f-score: 0.9965841161400513\n",
      "test f-score: 0.993570722057369\n"
     ]
    }
   ],
   "source": [
    "clf_lr_text_p = modelEval(distilBert_text_p, y_p, 0.33, lr, \"results_lr_text_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.998550284885877\n",
      "test accuracy: 0.9958932238193019\n",
      "train f-score: 0.998595918367347\n",
      "test f-score: 0.9960180514998673\n"
     ]
    }
   ],
   "source": [
    "clf_lr_title_text = modelEval(distilBert_title_text, y, 0.33, lr, \"results_lr_title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9997461123452872\n",
      "test accuracy: 0.9978092783505155\n",
      "train f-score: 0.9997560380580629\n",
      "test f-score: 0.9978989000123594\n"
     ]
    }
   ],
   "source": [
    "clf_lr_title_text_p = modelEval(distilBert_title_text_p, y_p, 0.33, lr, \"results_lr_title_text_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9607902633087219\n",
      "test accuracy: 0.9605749486652977\n",
      "train f-score: 0.9617169755423154\n",
      "test f-score: 0.961456102783726\n"
     ]
    }
   ],
   "source": [
    "clf_svm_title = modelEval(distilBert_title, y, 0.33, svc, \"results_svm_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.967185020628372\n",
      "test accuracy: 0.9686855670103093\n",
      "train f-score: 0.96811987420608\n",
      "test f-score: 0.9696212026503312\n"
     ]
    }
   ],
   "source": [
    "clf_svm_title_p = modelEval(distilBert_title_p, y_p, 0.33, svc, \"results_svm_title_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9856377060786892\n",
      "test accuracy: 0.9835728952772074\n",
      "train f-score: 0.9860428543345785\n",
      "test f-score: 0.9840234322992943\n"
     ]
    }
   ],
   "source": [
    "clf_svm_text = modelEval(distilBert_text, y, 0.33, svc, \"results_svm_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9871152015233259\n",
      "test accuracy: 0.9867268041237114\n",
      "train f-score: 0.9876045673810833\n",
      "test f-score: 0.9872666584250217\n"
     ]
    }
   ],
   "source": [
    "clf_svm_text_p = modelEval(distilBert_text_p, y_p, 0.33, svc, \"results_svm_text_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9941605839416059\n",
      "test accuracy: 0.9943298969072165\n",
      "train f-score: 0.9943847656249999\n",
      "test f-score: 0.9945558030190547\n"
     ]
    }
   ],
   "source": [
    "clf_svm_title_text_p = modelEval(distilBert_title_text_p, y_p, 0.33, svc, \"results_svm_title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9926502815144466\n",
      "test accuracy: 0.9921971252566735\n",
      "train f-score: 0.99287441982088\n",
      "test f-score: 0.9924302788844621\n"
     ]
    }
   ],
   "source": [
    "clf_svm_title_text = modelEval(distilBert_title_text, y, 0.33, svc, \"results_svm_title_text_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC #l2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "scores = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(distilBert_title_text_p, y_p, test_size=0.33, random_state=42)\n",
    "for c in range(1,5):\n",
    "    lsvmCV = LinearSVC(max_iter = 20000, C=c)\n",
    "    scores.append(cross_val_score(estimator=lsvmCV, X=X_train, y=y_train, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9979054467810812,\n",
       " 0.9979689791314156,\n",
       " 0.9979689791314156,\n",
       " 0.9979689791314156]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores #choose c=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(max_iter = 10000)\n",
    "lsvc2 = LinearSVC(max_iter = 20000, C=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9732982704561546\n",
      "test accuracy: 0.9679671457905544\n",
      "train f-score: 0.9740566037735848\n",
      "test f-score: 0.9688581314878894\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_title = modelEval(distilBert_title, y, 0.33, lsvc, \"results_lsvm_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9876864487464297\n",
      "test accuracy: 0.9774484536082474\n",
      "train f-score: 0.9881360078277887\n",
      "test f-score: 0.9784030605948414\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_title_p = modelEval(distilBert_title_p, y_p, 0.33, lsvc, \"results_lsvm_title_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9976062843464482\n",
      "test accuracy: 0.9909650924024641\n",
      "train f-score: 0.9976811783533099\n",
      "test f-score: 0.9912443618997081\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_text = modelEval(distilBert_text, y, 0.33, lsvc, \"results_lsvm_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy: 0.9945876288659794\n",
      "train f-score: 1.0\n",
      "test f-score: 0.9948122529644269\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_text_p = modelEval(distilBert_text_p, y_p, 0.33, lsvc, \"results_lsvm_text_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy: 0.9960301163586585\n",
      "train f-score: 1.0\n",
      "test f-score: 0.9961553758451545\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_title_text = modelEval(distilBert_title_text, y, 0.33, lsvc, \"results_lsvm_title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy: 0.997680412371134\n",
      "train f-score: 1.0\n",
      "test f-score: 0.9977755808205635\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_title_text_p = modelEval(distilBert_title_text_p, y_p, 0.33, lsvc, \"results_lsvm_title_text_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy: 0.997680412371134\n",
      "train f-score: 1.0\n",
      "test f-score: 0.9977755808205635\n"
     ]
    }
   ],
   "source": [
    "clf_lsvm_title_text_p_c2 = modelEval(distilBert_title_text_p, y_p, 0.33, lsvc2, \"results_lsvm_title_text_p_c2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8516570580897475\n",
      "test accuracy: 0.8499657768651608\n",
      "train f-score: 0.8537914534458695\n",
      "test f-score: 0.8528859060402684\n"
     ]
    }
   ],
   "source": [
    "clf_nb_title = modelEval(distilBert_title, y, 0.33, gnb, \"results_nb_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8703268803554427\n",
      "test accuracy: 0.8743556701030928\n",
      "train f-score: 0.8731607375675172\n",
      "test f-score: 0.8778654641112363\n"
     ]
    }
   ],
   "source": [
    "clf_nb_title_p = modelEval(distilBert_title_p, y_p, 0.33, gnb, \"results_nb_title_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9221199554971174\n",
      "test accuracy: 0.9243668720054757\n",
      "train f-score: 0.924809582709459\n",
      "test f-score: 0.9270193514298922\n"
     ]
    }
   ],
   "source": [
    "clf_nb_text = modelEval(distilBert_text, y, 0.33, gnb, \"results_nb_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9178673437004126\n",
      "test accuracy: 0.9252577319587629\n",
      "train f-score: 0.9189426208970183\n",
      "test f-score: 0.9268046441191318\n"
     ]
    }
   ],
   "source": [
    "clf_nb_text_p = modelEval(distilBert_text_p, y_p, 0.33, gnb, \"results_nb_text_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9204005259431577\n",
      "test accuracy: 0.920123203285421\n",
      "train f-score: 0.9226434258379477\n",
      "test f-score: 0.9223191106969314\n"
     ]
    }
   ],
   "source": [
    "clf_nb_title_text = modelEval(distilBert_title_text, y, 0.33, gnb, \"results_nb_title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9294827039035227\n",
      "test accuracy: 0.9371134020618557\n",
      "train f-score: 0.9311946491608348\n",
      "test f-score: 0.938877755511022\n"
     ]
    }
   ],
   "source": [
    "clf_nb_title_text_p = modelEval(distilBert_title_text_p, y_p, 0.33, gnb, \"results_nb_title_text_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, features, labels): #labels will be numpy (n,)\n",
    "        'Initialization'\n",
    "        self.features = torch.tensor(features, dtype=torch.long) #instance variables\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        X = self.features[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplr neural network to intake bert\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden = 700\n",
    "        self.l1 = nn.Linear(p, self.hidden, bias=True)  \n",
    "        self.l2 = nn.Linear(self.hidden, 2, bias=True) \n",
    "        \n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1, \n",
    "            nn.Tanh(), \n",
    "            nn.Dropout(), \n",
    "            self.l2, \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for training (similar to hw8)\n",
    "def train(num_epochs, dataloader, model, criterion, optimizer):\n",
    "    loss_curve = []\n",
    "    accuracy_curve = []\n",
    "    \n",
    "    for epoch in range(num_epochs): # loop over each epoch\n",
    "        epoch_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        for data in dataloader: # loop over each batch\n",
    "            embeddings, labels = data\n",
    "            outputs = model(embeddings.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save performance\n",
    "            epoch_loss+=loss.item()\n",
    "\n",
    "            _, preds = outputs.max(dim=1)\n",
    "            correct += preds.eq(labels.reshape(len(labels),)).sum() \n",
    "            total += float(len(labels))\n",
    "        \n",
    "        # calculate epoch stats and save epoch loss and accuracy\n",
    "        epoch_loss = epoch_loss / len(dataloader)\n",
    "        epoch_accuracy = correct / total\n",
    "        loss_curve.append(epoch_loss)\n",
    "        accuracy_curve.append(epoch_accuracy)\n",
    "        print('epoch [{}/{}], mean epoch loss:{:.4f}, epoch accuracy:{:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_accuracy))\n",
    "\n",
    "    return model, loss_curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets and dataloaders\n",
    "batch_size = 100\n",
    "\n",
    "dataloader_title = DataLoader(Dataset(distilBert_title, y), batch_size=batch_size, shuffle=True)\n",
    "dataloader_text = DataLoader(Dataset(distilBert_text, y), batch_size=batch_size, shuffle=True)\n",
    "dataloader_title_text = DataLoader(Dataset(distilBert_title_text, y), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloader_title_p = DataLoader(Dataset(distilBert_title_p, y_p), batch_size=batch_size, shuffle=True)\n",
    "dataloader_text_p = DataLoader(Dataset(distilBert_text_p, y_p), batch_size=batch_size, shuffle=True)\n",
    "dataloader_title_text_p = DataLoader(Dataset(distilBert_title_text_p, y_p), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate other inputs\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork(distilBert_text.shape[1])\n",
    "model_title_text = NeuralNetwork(distilBert_title_text.shape[1])\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "# criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/10], mean epoch loss:0.5167, epoch accuracy:0.7509\n",
      "epoch [2/10], mean epoch loss:0.4831, epoch accuracy:0.7745\n",
      "epoch [3/10], mean epoch loss:0.4797, epoch accuracy:0.7744\n",
      "epoch [4/10], mean epoch loss:0.4787, epoch accuracy:0.7751\n",
      "epoch [5/10], mean epoch loss:0.4760, epoch accuracy:0.7754\n",
      "epoch [6/10], mean epoch loss:0.4772, epoch accuracy:0.7770\n"
     ]
    }
   ],
   "source": [
    "# train models - only using text and text_title\n",
    "trained_model, loss_curve = train(num_epochs,dataloader_text,model,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Forest (Not working well, does not make sense to put in embeddings, discard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "scoresRF = []\n",
    "X_train, X_test, y_train, y_test = train_test_split(distilBert_title_text, y, test_size=0.33, random_state=42)\n",
    "for c in range(0,20):\n",
    "    rfCV = RandomForestClassifier()\n",
    "    scoresRF.append(cross_val_score(estimator=rfCV, X=X_train, y=y_train, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train_title, X_test_title, y_train_title, y_test_title = train_test_split(distilBert_title_p, y_p, test_size=0.33, random_state=42)\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(distilBert_text_p, y_p, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(distilBert_title_text_p, y_p, test_size=0.33, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "parameters = { 'max_depth':[15,16,17,18,19],'max_features':np.arange(8,10),\n",
    "              'n_estimators':[500, 1000, 2000],'min_samples_leaf': [5, 8, 10]}\n",
    "random_grid_title = GridSearchCV(rf, parameters, cv = 5)\n",
    "random_grid_text = GridSearchCV(rf, parameters, cv = 5)\n",
    "random_grid_text_title = GridSearchCV(rf, parameters, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_title_text = random_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_title = random_grid_title.fit(X_train_title, y_train_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_text = random_grid_text.fit(X_train_text, y_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_title_text.best_params_, rf_title.best_params_, rf_text.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=17, max_features=9, min_samples_leaf=10, n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9706011260577864\n",
      "test accuracy: 0.9264887063655031\n",
      "train f-score: 0.9713120147387815\n",
      "test f-score: 0.9280160857908847\n"
     ]
    }
   ],
   "source": [
    "clf_rf_title = modelEval(distilBert_title, y, 0.33, rf, \"results_rf_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9729609647730879\n",
      "test accuracy: 0.9418814432989691\n",
      "train f-score: 0.9738457760314342\n",
      "test f-score: 0.9438705662725576\n"
     ]
    }
   ],
   "source": [
    "clf_rf_title_p = modelEval(distilBert_title_p, y_p, 0.33, rf, \"results_rf_title_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9853679916388524\n",
      "test accuracy: 0.9647501711156742\n",
      "train f-score: 0.9857984293193718\n",
      "test f-score: 0.9655587507523574\n"
     ]
    }
   ],
   "source": [
    "clf_rf_text = modelEval(distilBert_text, y, 0.33, rf, \"results_rf_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9831799428752777\n",
      "test accuracy: 0.9641752577319588\n",
      "train f-score: 0.9838404780779317\n",
      "test f-score: 0.9655428854734756\n"
     ]
    }
   ],
   "source": [
    "clf_rf_text_p = modelEval(distilBert_text_p, y_p, 0.33, rf, \"results_rf_text_p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9893125653214659\n",
      "test accuracy: 0.9698151950718685\n",
      "train f-score: 0.9896266239078504\n",
      "test f-score: 0.9705862735943441\n"
     ]
    }
   ],
   "source": [
    "clf_rf_title_text = modelEval(distilBert_title_text, y, 0.33, rf, \"results_rf_title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9890193589336719\n",
      "test accuracy: 0.9698453608247423\n",
      "train f-score: 0.9894157234628327\n",
      "test f-score: 0.970953326713009\n"
     ]
    }
   ],
   "source": [
    "clf_rf_title_text_p = modelEval(distilBert_title_text_p, y_p, 0.33, rf, \"results_rf_title_text_p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_probas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Probability Calibration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
